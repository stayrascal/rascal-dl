import numpy as np

from sklearn.preprocessing import LabelBinarizer


y_labels = [7, 7, 10, 11, 10, 10, 10, 10, 10, 10, 10, 7, 4, 7, 7, 1, 7, 10, 7, 4]
x_train = [[1.89825666e-05, 3.47899121e-03, 2.60785618e-02, 4.28445208e-03,
            2.30720412e-04, 2.68236774e-04, 3.92102057e-06, 7.85597927e-01,
            2.92034284e-04, 1.39102188e-04, 1.68104874e-01, 1.15021962e-02],
           [4.44611542e-04, 1.20094759e-04, 3.75715423e-02, 6.11051209e-02,
            2.29513053e-04, 1.24840413e-04, 7.86298789e-05, 3.06122126e-01,
            2.33428940e-04, 1.37283306e-04, 5.89936731e-01, 3.89607794e-03],
           [1.93766550e-03, 4.60329095e-04, 3.32230744e-02, 2.19802933e-02,
            7.90979460e-04, 1.07273325e-04, 1.01452645e-04, 1.29552895e-01,
            7.27700005e-04, 5.92932268e-04, 8.06990748e-01, 3.53465626e-03],
           [2.70803948e-05, 7.59420480e-05, 1.78654100e-02, 5.71985874e-03,
            1.72922139e-04, 7.73848097e-04, 1.07010757e-05, 3.98376409e-01,
            3.71538951e-04, 4.09988474e-04, 1.88503996e-01, 3.87692305e-01],
           [2.40949762e-03, 1.82064563e-04, 7.22877715e-03, 6.12343963e-04,
            7.05937517e-04, 1.13606961e-02, 5.39850243e-03, 1.54226471e-01,
            5.72763170e-04, 8.59603096e-04, 8.16358121e-01, 8.52218154e-05],
           [1.69155711e-04, 6.21330360e-03, 1.35716685e-03, 5.31604032e-03,
            1.41128614e-02, 5.06793243e-03, 1.75061887e-03, 5.40687976e-01,
            1.20372472e-03, 2.28476038e-04, 4.15435057e-01, 8.45768729e-03],
           [2.04896831e-03, 2.15053527e-02, 9.47412793e-03, 2.88443288e-03,
            5.19096518e-02, 1.23075389e-02, 1.02706950e-02, 2.88449711e-01,
            1.76352794e-03, 1.79023142e-03, 5.87345015e-01, 1.02507471e-02],
           [3.87404530e-04, 7.48686170e-03, 2.78731057e-03, 5.21940751e-03,
            8.32725882e-02, 4.66154914e-02, 1.42340803e-02, 3.77220683e-01,
            7.28138081e-03, 1.11196355e-03, 4.33584238e-01, 2.07985910e-02],
           [9.99763182e-04, 2.30922158e-02, 6.17766765e-03, 2.39461278e-03,
            4.51195529e-02, 1.66358117e-02, 1.29760027e-02, 2.82893098e-01,
            2.40716068e-03, 1.68867037e-03, 5.98212311e-01, 7.40313325e-03],
           [3.99815179e-04, 1.70031758e-02, 2.05713673e-03, 1.47039556e-03,
            3.94770958e-02, 1.00845154e-02, 7.20255589e-03, 3.78263394e-01,
            3.16770383e-03, 7.78755753e-04, 5.33961758e-01, 6.13369780e-03],
           [2.42862261e-03, 2.48256517e-02, 5.73676229e-03, 6.25979620e-03,
            4.61714532e-02, 9.08610360e-03, 1.22921277e-02, 2.01767900e-01,
            4.10168573e-03, 1.66073387e-03, 6.77044615e-01, 8.62454888e-03],
           [1.71372714e-04, 5.09122422e-03, 1.85129441e-03, 1.55638560e-03,
            2.36792910e-02, 4.22606751e-03, 4.49760681e-03, 6.01572000e-01,
            8.13795526e-04, 3.12708884e-04, 3.52635367e-01, 3.59288627e-03],
           [2.42823651e-03, 6.36315068e-03, 3.60697919e-03, 1.03243573e-02,
            4.79442120e-02, 7.15442524e-03, 6.84094913e-03, 5.60668437e-01,
            2.39705075e-03, 6.13966075e-04, 3.39170816e-01, 1.24874198e-02],
           [3.59366601e-03, 1.39767924e-02, 3.61119171e-03, 4.49236617e-03,
            4.28169325e-02, 4.71054161e-03, 6.51602194e-03, 2.87240611e-01,
            3.47904872e-03, 1.51376359e-03, 6.17537069e-01, 1.05119957e-02],
           [1.52013246e-04, 7.69920442e-03, 2.03235699e-03, 4.60131156e-03,
            3.14496206e-02, 5.16085256e-03, 6.04925159e-03, 4.79559834e-01,
            8.87341769e-04, 1.74822496e-04, 4.57986175e-01, 4.24721583e-03],
           [2.50768430e-03, 1.80193620e-02, 6.65905690e-03, 3.28514098e-02,
            8.20882372e-02, 2.70102448e-02, 1.40531955e-02, 2.07732413e-01,
            7.37754294e-03, 1.13817816e-03, 5.66371786e-01, 3.41908894e-02],
           [1.07585486e-03, 1.25886000e-02, 1.53122583e-03, 1.64272663e-03,
            2.04384326e-02, 2.37611128e-03, 2.72796235e-03, 5.95625593e-01,
            9.99074528e-04, 3.25795193e-04, 3.56950938e-01, 3.71768638e-03],
           [1.64982232e-03, 1.36737683e-02, 2.25180109e-03, 2.86643825e-03,
            3.07649533e-02, 3.43747126e-03, 3.53043372e-03, 4.44766067e-01,
            1.93166170e-03, 4.39037289e-04, 4.88470187e-01, 6.21835795e-03],
           [9.57907911e-05, 2.62920804e-04, 2.22544015e-04, 5.55483593e-03,
            2.05237995e-04, 5.07963720e-03, 9.61188328e-03, 7.55421827e-01,
            2.14859967e-05, 9.35421693e-05, 2.14788709e-01, 8.64158509e-03],
           [1.73536443e-03, 1.38149862e-02, 7.51209460e-03, 3.11385111e-02,
            1.12468095e-01, 3.61158154e-02, 1.10132367e-02, 1.70654538e-01,
            1.03490910e-02, 1.58071232e-03, 5.74098588e-01, 2.95189683e-02]]


def dcg_score(y_true, y_score, k=5):
    print(len(y_true), len(y_score))
    order = np.argsort(y_score)[::-1]
    y_true = np.take(y_true, order[:k])

    gain = 2 ** y_true - 1
    discounts = np.log2(np.arange(len(y_true)) + 2)
    return np.sum(gain / discounts)


def ndcg_score(ground_truth, predictions, k=5):
    lb = LabelBinarizer()

    print('The length of predictions {}'.format(len(predictions)))
    lb.fit(range(len(predictions) + 1))
    T = lb.transform(ground_truth)

    print(T)

    scores = []

    for y_true, y_score in zip(T, predictions):
        actual = dcg_score(y_true, y_score, k)
        best = dcg_score(y_true, y_true, k)
        score = float(actual) / float(best)
        scores.append(score)
    return np.mean(scores)


ndcg_score(y_labels, x_train)